# Comparisons Lab

Head-to-head testing of AI models on the same tasks.

## Purpose

Don't just assume one tool is better—test it. This lab helps you:
- Discover which AI excels at what
- Build evidence-based tool selection
- Understand real-world performance differences
- Make informed decisions about when to use which

---

## Quick Start

1. Pick a task you need to do
2. Copy the comparison template
3. Run the same prompt on both Claude and Gemini
4. Score the results
5. Record your findings

---

## Comparison Template

```markdown
# Comparison: [Task Name]

**Date:** YYYY-MM-DD
**Task Type:** Coding / Writing / Analysis / Creative / Other

## The Task
[Describe what you're trying to accomplish]

## The Prompt
```
[Exact prompt used - identical for both]
```

## Results

### Claude
**Model:** Claude 3.5 Sonnet / Opus / Haiku
**Time:** [How long to respond]

**Output:**
[Summary or full output]

**Score (1-5):**
- Accuracy: ⭐⭐⭐⭐☆
- Completeness: ⭐⭐⭐⭐⭐
- Usability: ⭐⭐⭐⭐☆
- **Overall: 4.3**

### Gemini
**Model:** Gemini Pro / Flash / Ultra
**Time:** [How long to respond]

**Output:**
[Summary or full output]

**Score (1-5):**
- Accuracy: ⭐⭐⭐☆☆
- Completeness: ⭐⭐⭐⭐☆
- Usability: ⭐⭐⭐⭐☆
- **Overall: 3.7**

## Winner: [Claude / Gemini / Tie]

## Analysis
- Why did the winner perform better?
- What did each do well?
- What did each struggle with?

## Recommendation
For [this type of task], use [winner] because [reason].
```

---

## Scorecard

Track your findings over time:

| Task Type | Claude Wins | Gemini Wins | Ties | Notes |
|-----------|-------------|-------------|------|-------|
| Code Generation | | | | |
| Code Review | | | | |
| Debugging | | | | |
| Technical Writing | | | | |
| Creative Writing | | | | |
| Data Analysis | | | | |
| Summarization | | | | |
| Research | | | | |
| Math/Logic | | | | |
| Explanation | | | | |

---

## Decision Guide

Based on your comparisons, build your personal decision guide:

### Use Claude When...
- [ ] [Finding from your comparisons]
- [ ] [Finding from your comparisons]
- [ ] [Finding from your comparisons]

### Use Gemini When...
- [ ] [Finding from your comparisons]
- [ ] [Finding from your comparisons]
- [ ] [Finding from your comparisons]

### Either Works For...
- [ ] [Finding from your comparisons]
- [ ] [Finding from your comparisons]

---

## Comparison Ideas

### Coding
- [ ] Generate a REST API endpoint
- [ ] Debug a tricky async bug
- [ ] Review code for security issues
- [ ] Write unit tests for a function
- [ ] Refactor messy code

### Writing
- [ ] Technical documentation
- [ ] Blog post introduction
- [ ] Error messages
- [ ] Commit messages
- [ ] Code comments

### Analysis
- [ ] Explain a complex concept
- [ ] Analyze pros/cons of approaches
- [ ] Summarize a long document
- [ ] Research a technical topic
- [ ] Data interpretation

### Reasoning
- [ ] Multi-step logic problems
- [ ] Architecture decisions
- [ ] Trade-off analysis
- [ ] Edge case identification

---

## Tips for Fair Comparisons

1. **Same prompt** — Use identical prompts for both
2. **Same context** — Provide the same background info
3. **Fresh sessions** — Don't let prior conversation influence
4. **Blind scoring** — Score without looking at which is which (if possible)
5. **Multiple trials** — One test isn't enough for important decisions
6. **Note the model** — Specific versions matter

---

## File Organization

```
comparisons/
├── README.md (this file)
├── scorecard.md (running tally)
├── coding/
│   ├── 2024-01-15-api-generation.md
│   └── 2024-01-20-debugging-async.md
├── writing/
│   └── 2024-01-18-documentation.md
└── analysis/
    └── 2024-01-22-architecture-review.md
```

---

## Why This Matters

Opinions about AI tools are everywhere. Data is rare.

By systematically comparing, you'll:
- Cut through the hype
- Find what actually works for YOUR tasks
- Build expertise that transfers
- Make confident tool choices

Your comparisons are valuable—they're real evidence, not marketing.
